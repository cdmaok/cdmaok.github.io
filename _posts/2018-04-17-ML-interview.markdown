---
layout: post
title:  ML interview
description: some notes on machine learing interview
categories:
-blog
---

* 介绍一下SVM
  面向数据分类算法，确定一个超平面，从而将不同的数据分割开。 损失函数是hinge Loss
  一个终于让我看懂的blog http://blog.sina.com.cn/s/blog_4298002e010144k8.html

* 介绍一下tensorflow的计算图
  分成两个部分，define and run，构造部分，计算流图，然后通过session执行图中的计算。

* 欧式距离和曼哈顿距离
  欧式距离不解释，曼哈顿距离，假设你在曼哈顿从一个十字路口到另外一个十字路口，驾驶距离不是两个的直线距离，等于各等于各个维度上的差的绝对值的和

* 逻辑回归
  sigmoid 函数，对数损失函数。

* Overfitting
  解决方法 dropout,regularization,batch normalization？及早停止迭代，增加噪音数据，

* LR 和 SVM的区别
  相同：都是分类算法，可以添加各种正则项
  不同：损失函数不一样，svm是hinge loss，只考虑了分类最相关的少数点,比较不好理解，转化为对偶为题后，只需要计算与少数几个支持向量的距离，简化计算，支持多种核函数，，lr是对数损失函数，好理解。

* 决策树，RF，Boosting 和 Adaboost，GBDT和xgboost的区别
  决策树：Cart4.3 ID3
  RF：包含多个决策树
  Adaboost: 自适应增强
  GDBT：融合决策树和梯度上升boosting算法
  xgboost:GBDT的提升，损失函数是泰勒展开二项逼近，不是一阶导数，树进行了优化，之所以这样做可以不限定损失函数。

* 判别式模型和生成式模型
  判别式：直接通过函数进行判断，
  生成式：学习联合概率密度，然后求条件概率

* l1 和l2正则化
  l1: 因子绝对值之后，使得权值更加稀疏，方便提取特征，符合拉普拉斯分布
  l2: 因子平方和开更号，防止过拟合，符合高斯分布

* CNN works：
  存在局部和整体的关系，由低层次的特征通过组合，得到高级特征并得到不同特征组合的空间关系性。
  主要是局部连接/权值共享/池化操作（取最大或者均值之类的操作）/多层次结构

* lstm 和 rnn
  lstm中包含forget gate，input gate，cell state，hidden information等，lstm是累加形式，rnn是累乘，可以防止梯度消失，或者梯度爆炸，nn隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出

* 搜索联想 贝叶斯法则讲述

* plsa 和 lda
  lda是plsa的贝叶斯版本，plsa用极大似然的方法进行求解，lda把主题分布和词语分布弄成随机向量加入dirichlet先验。

* em算法 gmm模型
  e：选取一组参数，求出该参数下隐变量的条件概率值
  m：根据条件概率，推导出联合似然函数的最大值

* 为什么要做归一化
  1.归一化可以提高梯度下降法求解最优解的速度
  2.提高精度，不同特征之间量纲不一样，如果使用距离计算公式会出问题。

* 机器学习流程：
  1.抽象成数学问题，分类还是聚类，
  2.获取数据
  3.特征预处理和特征选择
  4.模型调优和诊断，是否过拟合等等
  5.线上线下融合
  6.上线

* 逻辑回归离散化特征
  工业界很少将连续特征值作为逻辑回归模型的特征输入，而是将连续值离散化，这么做的原因主要是：
  1.离散特征易于添加或者去除
  2.稀疏向量内积快
  3.分桶操作后对异常值有很强的鲁棒性
  4.离散化之后表达能力变强

  * 数据结构分析
  数组：运行速度快，存储效率高，比较适合查找操作
  链表：适合curd，增删改查
  二叉树：具有层次关系的问题

  * 对抗生成网络
  有两方，一方是generator，主要工作是生成图片，另一方是discriminator判断图片是否属于真实样本，双方相互提升。

  * hashtable 和 hashmap
  hashmap基于hashtable实现，hashmap是非同步的，允许null值。


  * deep learning 调参技巧
  1.首先参数初始化，unifrom或者normal高斯分布
  2.数据预处理，进行标准化操作
  3.relu + bn

  * 标准化和归一化区别
  标准化：矩阵 - 均值 / 标准差
  归一化：实质上将最大值映射为1

  * lstm上的sigmoid 和 tanh区别
  二者目的不一样：sigmoid 用在了各种gate上，产生0~1之间的值，这个一般只有sigmoid最直接了。tanh 用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。  @hhhh：另可参见A Critical Review of Recurrent Neural Networks for Sequence Learning的section4.1，说了那两个tanh都可以替换成别的。


  * 梯度消失和梯度爆炸
  消失：根据链式法则，对底层求导=输出对上层求带*权重 联乘，如果由于sigmoid函数的倒数的最大值为1/4，并且w通常会小于1，那么底层的梯度会越来越小，可以通过relu来避免这一情况
  爆炸：如果w在这个时候>>1的话，也会出现这个问题。

  * svd & pca
    PCA是使得数据投影之后方差最大，经过去除均值的操作后，可以用SVD来求解这一向量。

  * 数据不平衡问题
  1.降采样或者过采样
  2.直接加权
  3.修改损失函数
  4.改变评判标准，使用AUC，ROC等
  5.使用bagging，boosting
  6.考虑数据先验分布


  * 深度学习方法脉络
  DNN全连接 --> 解决全连接 CNN，--> 解决时许问题 rnn --> 解决梯度消失问题 --> lstm
  sigmoid会饱和，造成梯度消失。于是有了ReLU
ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。
强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。
太深了，梯度传不下去，于是有了highway。
干脆连highway的参数都不要，直接变残差，于是有了ResNet。
强行稳定参数的均值和方差，于是有了BatchNorm。
在梯度流中增加噪声，于是有了 Dropout。
RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。
LSTM简化一下，有了GRU。
GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。
WGAN对梯度的clip有问题，于是有了WGAN-GP。

* 线性分类器
感知准则函数 ：准则函数以使错分类样本到分界面距离之和最小为原则。其优点是通过错分类样本提供的信息对分类器函数进行修正，这种准则是人工神经元网络多层感知器的基础。
支持向量机 ：基本思想是在两类线性可分条件下，所设计的分类器界面使两类之间的间隔为最大，它的基本出发点是使期望泛化风险尽可能小。（使用核函数可解决非线性问题）
Fisher 准则 ：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。

* bagging & boosting
bagging 均匀采样，各个预测函数没有权重，可以并行
boosting 按照错误率采样，各个预测函数有权重，无法并行。
