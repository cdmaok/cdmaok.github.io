---
layout: post
title:  Hive Note
description: Hive
categories: blog
---

本篇主要是《hive编程指南》的笔记。

* Hive主要是提供了一个用SQL方式来执行MAPREDUCE任务的工具。这里同时介绍几个Hive相关的替代工具。
* Pig -- 假设用户的输入数据有一个或者多个源，而用户需要进行一组复杂的变换来生成一个或者多个输出数据源。如果使用Hive，用户可能会使用嵌套查询来解决这个问题，但是在某些时刻会需要重新保存临时表。ping使用流式语言，使用这种语言会比一组复杂的查询更加直观。
* HBase --Hive无法提供行级别的更新，快速的查询响应时间以及支持事务等，Hbase用来提供这些功能。


```shell
hive cli command
hive -e sqlcommand hive 执行sql命令
hive -f xx.sql hive 执行sql脚本
hive -s 保存结果到文件
```


* hive 数据类型
tinyint, smallint,int,boolean,float,double,string,timestamp,binary


* 创建数据库表

    ```SQL
    create table if not exists mydb.employees(
        name string comment 'employee name',
        salary float comment 'employee salary',
        subordinates array<string> comment 'name of subordinates',
        deductions map<string,float> comment 'xxxx',
        address struct<> comment 'xx'
    )
    comment 'description of the table',
    tblproperties('creator'='me'),
    location '/user/hive/warehouse/mydb.db/employee'; ```


describe extended mydb.employee  

* 创建数据库表并分区

```SQL
create table employee(
    name string,
    salary float,
    subordinates array<string>，
    deductions map<string,float>
)
partition by (country string,state string);
```

show partitions employee 查看分区表中的所有分区。

通过设置hive.mapred.mode=strict,这样如果对分区表进行查询而where子句没有加分区过滤的话，将禁止提交这个任务。

定义文件格式
```SQL
create table employees(
    name string,
    salary float,
    subordinates array<string>,
    address struct<street:string,city:string,state:string,zip:string>
)
row format delimited
fields terminated by '\001'
collection items terminated by '\002'
map keys terminated by '\003'
line terminated by '\n'
stored as textfiles
```

* 向管理表中加载数据
```SQL
load data local inpath '${env:HOME}/california-employee'
overwrite into table employee
partition(country = 'US',state='CA')
```

* 通过查询语句向表中插入数据  
```SQL
insert overwrite table employee
partition (country='US',state='OR')
select * from staged_employee se where se.cnty='US' and se.st='OR';
```

* 动态分区插入
```SQL
insert overwrite table employees
partition by(country,state)
select ...,se.cnty,se.st
from staged_employees se;
```

* 单个查询语句并创建表
```SQL
create table ca_employee
as select name,salary,address
from employee
where se.state='CA'
```

* 导出语句
hadoop fs -cp source_path local_path


* hive中一些内置的函数

返回值类型|样式|描述
--|--|--
bigint|round|四舍五入
bigint|floor,ceil|取上限或者下限
double|tan|正切
double|variance|计算方差
double|var_samp|计算一组数组的样本方差
double|corr(col1,col2)|返回两组数值的相关系数
array<struct<'x':'y'>>|historgram_numeric|返回NB数量的直方图仓库数组，返回结果中的值x是中心，y是仓库的高
array|collect_set(col)|返回集合col元素排重后的数组
tuple|parse_url_tuple()|从url中解析出N个部分信息
string|ascii()|获取首个字符的整数值
string|concat()|拼接字符串
string|concat_ws()|按照指定分隔符进行拼接
int|find_in_set()|返回在以逗号分隔符中s出现的位置
string|repeat()|重复输出n次字符串s
int|year/month/day()|返回时间中的年月日

select parse_url_tuple(url,'HOST','PATH','QUERY') as (host,path,query) from url_table;

* join 操作
hive支持普通的sql join操作，但是只支持等值连接。
1. 当对三个或者更多表进行join连接时，如果每个on子句都使用相同的连接键的话，那么只会进行一个map reduce job
2. hive同时嘉定最后一个表是最大的表，在对每行记录进行连接操作时，他会尝试将其他表缓存起来，然后扫描最后那个表进行计算。
3. 由于hive中不存在in,exist 语句，需要通过left semi join左半开连接代替，例如
```SQL
    select s.ymd,s.symbol,s.price_close from stocks s where s.ymd,s.symbol in (
        select d.ymd,d.symbol from dividends d
    )
    ==>
    select s.ymd,s.symbol,s.price_close from stocks s left semi join dividends d on
        s.ymd = d.ymd and s.symbol = d.symbol
```

* order by 全局排序 & sort by 局部排序（每个reduer里面排序） & distribute by 控制map的输出在reduce中如何划分 & cluster by 等价于 sort by 叠加 distribute by

* 建立索引  

    ```SQL
    create index employee_index
    on table employee(country)
    as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'
    with deferred rebuild
    idxproperties ('creator'='me')
    in table employee_index_table
    partitioned by (country,name)
    comment by 'sth'
    ```

* 解释mapreduce语句执行
    explain select sum(number) from onecol;

* limit 语句在hive中会经常用到，但其实没有什么卵用，还是会跑完这个过程。


* show functions; # 列举函数
* describe function concat; # 展示说明文档

* 自定义UDF函数  
```java
import org.apache.hadoop.hive.ql.exec.UDF:

public class testUDF extends UDF{

    public String evaluate(String ...){}
    // 必须实现这个函数
}
```
然后添加到hive中
```sql
hive> add jar xx.jar
hive> create temporary function xx as 'class path'
! 这样只在一个session中有效，每次需要添加，长期需要使用的话加入到$HOME/.hiverc中去自动执行
```

* hive  streaming
```sql
hive> select transform(a,b)
    > using '/bin/sed s/4/10'
    > as newA,newB from a;
    
    > add file ${env:HOME}/xx.sh
    ! 添加脚本到所有节点中去。
```

* 一个word count 的streaming 例子
```sql
hive> create table docs(line string);
    > create table word_count(
        word string,
        count int
    )
    row format delimited fields terminated by '\t'

    > from(
        from docs 
        select transform(line) using 'xx/mapper.py'
        as word,count
        cluster by word) wc
      insert overwrite table word_count
      select transform(wc.word,wc.count)
      using 'xx/reducer.py'
      as word,count
```