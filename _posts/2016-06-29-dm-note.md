---
layout: post
title: dm-note
category: ML
description: 机器学习面试准备
---

一个简单的路线图
![ml-route](http://7xpv97.com1.z0.glb.clouddn.com/4c02759b1997c08e754b915c1f61506a.png)

regression and classification; 回归返回的是一个连续值，分类返回的是一个离散的值，regression 可以转化成 classification 问题

### Linear Model

#### LR
名字听起来是一个回归问题，但实质上是一个分类算法，把 线性模型得到的回归值(wx) 通过sigmoid 函数转化成一个[0,1]之间的概率，**无需事先预设数据分布，避免了假设分布不准确所带来的问题，不仅仅预测出类别，还可以得到近似概率预测，可以辅助决策**


最小平方误差SE等价于残差符合高斯分布正态分布脩下的最小NLL。而
最小绝对值误差AE等价于残差符合拉普拉斯分布下的最小NLL

![loss-function](http://7xpv97.com1.z0.glb.clouddn.com/d78528fd66a6872a452308c2a5949abd.png)

![optimial solution](http://7xpv97.com1.z0.glb.clouddn.com/d59449e0babb3a59b2f2bbac38ce545c.png)

#### LinearRegression
线性模型，回归， 没有任何保护，直接求解MSE，预测时用模型和输入做点乘。
[求解过程](http://www.netlib.org/lapack/lug/node27.html)

#### Ridge Regression
y = xw, SE + L2 正则化，回归，GD，SGD BFGS L-BFGS

#### Lasso
y = xw, SE + L1 正则化，回归，见上图

#### Elastic Net
y = xw, SE + L1 + L2 正则化，L1 和 L2以一定比例混入， 求解待查。


#### Multi-task Lasso
MTL 多任务学习， SE + 正则化， 三维， 注意一个二维矩阵的正则化是L1/L2混合范式，首先是列向量做L2范式，然后对行向量L1范式,值得注意的是MTL中的权值矩阵W有可能做各式各样的假设，例如一部分稀疏特征的权重加上一部分不稀疏特征，分别对这两部分特征进行分别的正则化。更多可以参考[这里][https://www.siam.org/meetings/sdm12/zhou_chen_ye.pdf]，一个给出的[工具包](www.public.asu.edu/~jye02/Software/MALSAR/)


### Least-angle regression
一个用来处理高维版本的LinearRegression模型，方法和特征选择的方法类似，从0开始，每次选取一个特征，如果这个特征的相关系数和残差的方向更加靠近，那么将这个特征加入到这个模型

### LARS Lasso
LARS + L1 正则化

### Orthogonal Matching Pursuit
一个和LAR基本类似的算法，给定一个过完备字典矩阵，其中它的每列表示一种原型信号的原子。给定一个信号y，它可以被表示成这些原子的稀疏线性组合。信号 y 可以被表达为 y = Dx ，MP算法的基本思想：从字典矩阵D（也称为过完备原子库中），选择一个与信号 y 最匹配的原子(也就是某列)，构建一个稀疏逼近，并求出信号残差，然后继续选择与信号残差最匹配的原子，反复迭代，信号y可以由这些原子来线性和，再加上最后的残差值来表示。如果信号(残值)在已选择的原子进行垂直投影是非正交性的，这会使得每次迭代的结果并不少最优的而是次最优的，收敛需要很多次迭代。
OMP算法的改进之处在于：在分解的每一步对所选择的全部原子进行正交化处理，这使得在精度要求相同的情况下，OMP算法的收敛速度更快。详细见[1](https://etd.lib.msu.edu/islandora/object/etd%3A1711/datastream/OBJ/view),[2](http://blog.csdn.net/scucj/article/details/7467955)

### Bayesian Regression
贝叶斯回归，线性模型，认为 y 在Ax的基础上符合高斯分布
![bayesian regression](http://7xpv97.com1.z0.glb.clouddn.com/34a8585945f59e7ec5fc3dbef9ee40ec.png)


#### Bayesian Regression Ridge
Bayesian Regression + prior(正好是L2正则化)

#### Automatic Relevance Determination
在 Bayesian Regression Ridge 之上换了一个prior，prior不是一个单位阵的n倍，而是一个对角阵生成

### Logistic regression
线性模型加sigmoid函数（线性空间到指数空间的映射），配上L1,L2正则化。不同的正则化配上不同的解法

### Stochastic Gradient Descent
由于求解过程中往往会用牛顿法之类的方法，而牛顿法每次迭代的过程中计算梯度都要使用整个数据集，为了加速算法，分成batch进行，每次只计算一个batch的梯度作为整个数据集的梯度。在sklearn中集成了SVM使用SGD和LR使用SGD的解法（通过损失函数进行区分），同样也可以解决回归问题，同理根据损失函数进行区分回归算法

### Perceptron
一款最简单的神经网络，y = sign(Ax), 可以加正则（待研究），错分时才更新向量。

### passive-aggressive algorithms
等同与 Perceptron, y = sign(Ax), 可以加正则吧（sklearn 看起来不支持），根据不同的loss function 切换不同的学习率


### Robustness regression
一些**抗噪，抗离群**的回归算法。

####  随机抽样一致算法（RANdom SAmple Consensus,RANSAC）
>在数据中随机选择几个点设定为内群
计算适合内群的模型
把其它刚才没选到的点带入刚才建立的模型中，计算是否为内群
记下内群数量
重复以上步骤多做几次
比较哪次计算中内群数量最多，内群最多的那次所建的模型就是我们所要求的解

#### Theil-Sen estimator
也是一个降噪算法，给定一个参数N_sample，每次都从整个数据集N中随机采样出大小为N_sample的一个子集，然后在整个子集上训练出一个模型，可以设置一共生成多少个模型，最后根据SE或者其他标准选择一个最后的模型。

### Polynomial regression  
![polynomial representation](http://7xpv97.com1.z0.glb.clouddn.com/7a4b8c9c48f38563e80495ca12ba27d9.png)
如图所示，多项式模型可以通过线性模型加一个空间的映射完成，在sklearn中，如果想用多项式模型，可以使用Polynomial regression 首先做一个映射，然后用映射后的输入（此时为线性模型）完成线性模型的训练，PolynomialFeatures 作为一个adapter 存在。

### Linear Discriminant Analysis
线性判别分析  
![Linear Discriminant Analysis](http://7xpv97.com1.z0.glb.clouddn.com/e6422decebc80dd12f0b79e0cdd0599c.png)  
找一个直线，令所有的数据点都投影到这根直线上面，使得同类的数据在投影线上尽可能靠近，不同类的点尽可能离得远。训练得到这根直线之后，对新来的点计算到各个标号的聚类判断标号。**这个算法可以作为降维的方法出现，每进行一次，降维一次**，解法包括SVD，lsqr，eigen

### Quadratic Discriminant Analysis
和LDA一样，不同点是认为不同类的协方差矩阵不一样，求解方法在sklearn中也有不同，用的是贝叶斯观点。

### Kernel ridge regression
Kernel 方法 + Linear + l2 正则化，类似于 **SVR**,不同的是损失函数，这里用的是SE

### Support Vector Machines
在训练集D在样本空间中找到一个划分超平面，将不同类别的样本分开。  
![SVM](http://7xpv97.com1.z0.glb.clouddn.com/673bb14976bfd20f9a30278c45b77354.png)
sklearn 中所有关于SVM的代码都是用的libsvm或者liblinear,请参阅这两部分的源代码

#### classification
一般而言，通常意义上的SVM都应该指的是hinge loss + l2 正则化的这个版本（支持软间隔），sklearn 中有三个关于SVM classification的实现，分别有不同的功能，有的是原生的mulit-class,有的用one vs one，有的只提供线性核函数。

#### regression
和分类一样，也是期望找到一个超平面，y= wx + b,但是损失函数稍有不同，只要真实的y和预测的y在eplison 之间，认为还是判断很正确的。

### Unsupervised Nearest Neighbors
无监督寻找近邻，除了暴力检索以外，主要包含两个方法，一个是KD-tree, 一个是Ball Tree 算法，两个算法基本类似，都是将空间划分成一个树状的结构，KD-tree 的每个节点是一个方体或者矩形，Ball Tree 的节点主要是一个圆形或者球体。算法描述如下：  
每次选取一个方差最大的维度（数据最为分散），选该维度上的中位数，低于这个中位数和高于中位数的分别组成两个子集，然后递归往下，直到每个子集里面只包含一个数据点。
关于这三种方法的选取有一定的原则：1.数据集小的话直接暴力搜索；2.如果数据相对稀疏的话可以选择两种tree-based方法，越稀疏，Ball Tree 算法提升越快；3.需要检索的的近邻个数，如果k越大，tree-based 方法也会越来越慢，一来需要遍历的空间变大，而来需要回溯的节点也更多，空间可能吃不消。

#### Nearest Neighbors Classification
直接根据K近邻的类标号投票的分类器。可以用上述的三种方法加速这个过程

#### Nearest Neighbors Regression
同样根据k近邻的回归值进行预测。具体是不是投票要看代码。

#### Nearest Centroid Classifier
简单分类算法，记录每个类的中心点即可。分类时计算输入和每个类中心的距离进行判断，简单的优化是设置阈值，如果有些离群的特征值超过了阈值，可以进行截断。

### Approximate Nearest Neighbors
如果数据的维度过高（50以上），上述寻找近邻的方法时间复杂度很高，为了加速，需要近似的寻找近邻的方法。一个有趣的思路是使用LSH，可以快速的挑选出候选对，但是有一个问题就是找候选对的时候需要给定一个阈值，万一query是一个远离中心的点，那么它的阈值可能需要比较低才能找到候选对，这样一来就需要相当多的LSH index,为了解决这个问题，使用一个LSH Forest，把每个LSH index 转成一个LSH hash signature 的前缀树，如图所示，注意这是一个二叉树。l个这样的树构成一个forest
![lsh forest](http://7xpv97.com1.z0.glb.clouddn.com/0a978a07bf1f4287c7949848ef93cb41.png)
查询操作如下：对一个query 生成一个hash signature,然后根据前缀树找到一个跟query 有最长前缀匹配的叶子节点的hash signature，那么query的k近邻应该跟这个具有最长共同前缀的叶子节点的哈希签名更加相似，把每棵树里面有共同最长前缀的点当做候选点，计算真实的相似度，排序返回，相似可以看[这里](http://infolab.stanford.edu/~bawa/Pub/similarity.pdf)


### Gaussian Processes
本质上是一个后处理的adapter,可以套在回归模型上，给出一个预测结果的置信区间，但是对于高维数据和稀疏的空间不友好。数学公式如图所示：  
![gpml](http://7xpv97.com1.z0.glb.clouddn.com/af1240bd44c8c89055ce137db0332b71.png)
在一个回归模型上套一个高斯分布（zero-mean），用SE预估出beta和回归模型的参数。

### Cross decomposition
互分解，可以计算不同数据集上面不同feature的协方差矩阵。给定一个场景，Y=AX，如果X，Y是满秩的，那么模型的参数A是容易计算的；退而求此次，可以对X做主成分分析，然后替换X，这时X就是满秩的；这个时候会有一个问题，筛选出来的X的主成分并不一定和Y相关，通过互分解找到和Y协方差更大的feature可以避开这个问题。具体看[这里](https://www.utdallas.edu/~herve/Abdi-PLS-pretty.pdf)

### Naive Bayes
如图上推导公式，求解过程就是MAP
![naive bayes](http://7xpv97.com1.z0.glb.clouddn.com/7df4fadce7774f882a193384d12c1e62.png)
由此生成的各种贝叶斯修改生成假设
例如Gaussian Naive Bayes
![Gaussian Naive Bayes](http://7xpv97.com1.z0.glb.clouddn.com/1aa5a73e9c8ab248b85194b921b21b8b.png);  
还有多项式分布，二项分布等等。这种基于概率的算法也需要load这个数据集，可能会导致内存问题，和SGD一样的思路，可以将数据分batch，变成一个online的算法

### Decision Trees
决策树，可以做分类和回归。抄一些优点和缺点。  
**优点**：可以可视化；需要一些简单的预处理，包括正则化，特征缺失的处理方法；数据点和效率成对数增长；可以同时处理number feature和 categorical feature。**缺点**：容易过拟合；鲁棒性不好；可能不是最优解；不平衡数据也不好处理

#### DT classification
每次从数据集D中选择最优划分属性，将数据D分成两个部分，每次再递归向下继续构造直到完成树的构造。根据选择划分属性的函数不同有两种标准算法，一种是使用Entropy的ID3算法，一种是使用Gini的CART算法

#### DT regression
认为所有属性独立的，选择一个属性，然后选一个划分点，可以得到两个区域，分别计算这个区域内所有点和区域均值的均方差（SE），如果最小，那么这个划分点成立，接着向下构造，接受输入的时候，返回这个区域的均值。**意味着两个不同的点如果划分到同一个区域，返回值是一样的**

### Ensemble Method
集成学习，主要包括两个方向，一个是averaging method,构造很多分类器，用它们的平均预测结果来作为最终结果，以消除单个分类器的bias；另外一种是boosting算法，持续不断的构造分类器来修正之前错分的样本的效果。经验上看，bagging和一些复杂的分类器搭配效果更好如（DT），boosting和一些弱分类器搭配效果更好

#### Bagging
每次都有放回的抽样，并行训练n个分类器，最后投票生成结果。可以对样本进行抽样，同样也可以对特征进行抽样。

#### Random Forest
随机森林，经典算法之一，把bagging的base estimator 换成了深度自定义的DT，而且特征不是从全局里面选的，而是K个全部特征里选任意d个，然后从d个里面任意选的。不仅有样本扰动，还有特征扰动。
分类问题返回所有树的投票结果，回归问题返回所有树的均值

#### Extremely Randomized Trees
基于RF的改进，在RF中，每次都是从一个子集里用每个特征的最好划分点计算，然后得出最优特征进行下一步，ET中是从子集里面选每个特征的随机划分点计算，然后得出最优的特征进行划分。可以看[这个](http://www.montefiore.ulg.ac.be/~ernst/uploads/news/id63/extremely-randomized-trees.pdf)

#### Totally Random Trees Embedding
这个东西可以当Embedding用，就是把输入在一堆训练好的树上面的路径表示成一个向量

#### AdaBoost
串行算法，每次提升错分样本的权重（修改错分样本在原有训练集里面的分布），重新训练分类器，最后对所有分类器的结果加权组合求符号表示结果。

#### Gradient boosting
包括分类和回归两种任务；优点是处理混合特征，包括numeric和categorical features; 鲁棒性很好； 缺点是boosting是串行的，比较慢。思路也是boosting加 tree 作为base estimator, 每一轮加入一棵树，建立目标函数SE + L2正则化，  每次根据目标函数选feature和split point建立新的树。[详细](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)

#### Voting Classifier
投票分类器，相当于也是adapter,会调用base estimator 的fit方法获得的预测分类值，如果是hard voting，那么则是一个estimator一票，如果是soft voting，那么是一个estimator有一定的权重，最后可以是票数可以是小数

### multi-class, mulit-label classification
 多分类和多标签分类都不再复述。记一下原生支持多分类的多分类算法：Naive Bayes; LDA and QDA; DT: RF; Nearest Neighbors; LR;原生支持多标签的算法： DT; RF; Nearest Neighbors; Ridge Regression;
 除此之外，其他的分类算法需要one vs one 或者 one vs rest 进行适配支持。one vs one 分类器比较多，但是需要的数据少，总体来说这两种方法效果差不多

#### one vs one
对任意两种类标号建立一个base estimator，共有n(n-1)/2种结果，最后投票出一个结果

#### one vs rest
对每个标号和其余标号建立一个base estimator,共有n个分类器，根据每个分类器的置信程度获取最终结果，这样一来，需要分类器提供置信程度的接口

#### Error Correcting Output Codes
如图取自《机器学习》
![ecoc](http://7xpv97.com1.z0.glb.clouddn.com/5617500e0325864decaf021fd6d00e2a.png)  
首先是编码： 对N个类做M次划分，将每个数据集分成一部分正例，一部分反例，训练出一个分类器，这样一共就有M个分类器和M个训练集和已知类标号的结果编码
然后是解码： M个分类器都对输入进行分类，可以得到一个结果向量，和已知类的编码进行相似度计算，取最相似的结果

### Feature Selection
如果特征很多，可以考虑进行一下特征选择。一种简单的算法是计算特征的方差VarianceThreshold，如果方差小于某个阈值，没有区分能力；还有通过其他函数例如卡方检验等，计算各个特征得分的SelectKBest；一种是从上至下，每次从全局去掉一部分，依据是系数或者权重；*压缩感知***

#### L1 regularization
在模型上面加L1正则化模型，这样一来倾向于选稀疏解，所有系数为0的特征都可以被去掉

#### Randomized sparse models
如果本身是一个相对稀疏的模型，用了L1正则化之后，一组相当接近的特征被筛选剩下一个（可能因为数据的问题扰动了），如果想避开这种问题，就对训练集进行采样，每一采样都重新计算特征的系数；总的来说，这不是最优选择，而是一种稳定的降维。

### 半监督学习
有标记的样本太少，可以希望分类器一边标注数据一边学习数据，常用的算法有EM算法，S3VM等，主要思想是通过吸收已标记附近的未标记点，通常认为两个点越相似，那么这两个点是同一个标记的可能性越大。

### Isotonic regression
保序回归，也可以称为分段回归，跟CART类似。如图所示。
![isotonic regression](http://7xpv97.com1.z0.glb.clouddn.com/3835694370782b206b9837135788a0a2.png)

### Probability calibration
概率校准，首先总结一下哪些分类器给出分类结果的同时给出概率，首先是LR(sigmoid + 线性模型)，其次是Naive Bayes, RF Classifier(每棵树的结果组成一个向量，然后进行正则化可以得到概率)；最后是linear SVC(sigmoid + 线性模型)，这也是分类模型的adapter。
以下给出一个概率校准的例子。
如图是一个数据分布  
![probability calibration dataset](http://7xpv97.com1.z0.glb.clouddn.com/ba127b16a5e259a7bccd7922cca11f37.png)
中间第二个团属于两个类的概率应该是0.5，给出一个校准的例子,通过分段回归就可以解决这个问题
![probabilty calibration curve](http://7xpv97.com1.z0.glb.clouddn.com/04022999dd11c21d0615731f00107aa7.png)
以下是一个展示校正方向的例子。
![calibration example](http://7xpv97.com1.z0.glb.clouddn.com/282af1691bdb91ec3c4f711f1e7c3455.png)

### Gaussian mixture models
高斯混合模型，混合学习模型中最快的一种，需要事先给出混合模型的个数；通常用EM算法求解，首先给出一组高斯分布，然后判断每个点组成比例，然后重新迭代确定高斯分布，重复N次，一般用EM算法求解；还有用变分推断求解的VBGMM（加了正则化），还有DPGMM(和变分推断解法一致，无需预设主题个数)

### Manifold learning
中文名字叫做流行学习，是用来针对非线性结构数据的降维，区别于PCA，LDA等。两种方法分别是Isometric Mapping 和  Locally Linear Embedding。前者可以理解理解成是MDS或者带核函数的PCA，在低维度上面找到一种映射方法，可以维持两个点在高维度上面的距离。LLE的思想就是，一个流形在很小的局部邻域上可以近似看成欧式的，就是局部线性的。那末，在小的局部邻域上，一个点就可以用它周围的点在最小二乘意义下最优的线性表示。LLE把这个线性拟合的系数当成这个流形局部几何性质的刻画。那末一个好的低维表示，就应该也具有同样的局部几何，所以利用同样的线性表示的表达式，最终写成一个二次型的形式，十分自然优美。还有很多其他的方法，需要补充黎曼集合的知识，在此按下不表，可以参考[这里](http://blog.csdn.net/chl033/article/details/6107042)

### clustering
聚类是ml里面很大的一个分支，简单的记录一下。

#### K-means
最简单的聚类算法，使用EM求解，首先设K个点为簇中心，其他点根据距离赋值，然后重新计算簇的中心，重复多次直到簇中心不再改变。缺点是对簇的形状有要求，如果是环形簇或者是别的形状效果不好；需要预设簇的个数；容易遭受维数灾难；容易局部最优，需要用不同的初始化值多跑几次(K-means++);一种简单的加速的方法是MiniBatchKMeans，每次只修改一个batch的cluster赋值

#### Affinity Propagation
近邻传播，一种比较新的聚类方法，不需要预设簇的数目，是另外的两个参数；时间复杂度较高o(n^2*t);具体算法描述见图。
![AP description](http://7xpv97.com1.z0.glb.clouddn.com/e6ff60d00d257026dadd9f7b13a4977b.png)
解释几个名词，r(i,k)表示k点适合作为i点的质心的合适程度，a(i,k)表示k对i点选它作为质心的吸引力，s是固有的偏好程度（相似度），**可能很合适，但是吸引力不够，这种事情常有发生**
合适程度 = 相似度 - 其他点对i的最大吸引程度，吸引程度 = 本身的合适程度 + 对其他点的合适程度。形象的解释[在知乎](https://www.zhihu.com/question/25384514)
初始值都是0，交替更新后选择 a + r 最大的点k作为i的质心，最后总结一下即可。

#### Mean Shift
均值转移，也是一个不需要预设数目的聚类算法，而是基于聚类密度的算法。 需要先实现k近邻的搜索算法来加速，首先任意选几个点作为质心，然后把这个点周围的点（给定一个距离）都认为是一个类，计算所有这些点导致的中心的偏移，重复多次，如果质心的变化很小可以认为已经收敛结束

#### Spectral clustering
中文名字叫谱聚类，简单理解就是对相似矩阵降维之后用K-means 算法求解的一种手段。效果据说不错；需要预设主题数目。算法描述如下：
>根据数据构造一个 Graph ，Graph 的每一个节点对应一个数据点，将相似的点连接起来，并且边的权重用于表示数据之间的相似度。把这个 Graph 用邻接矩阵的形式表示出来，记为 W 。一个最偷懒的办法就是：直接用我们前面在 K-medoids 中用的相似度矩阵作为 W 。
把 W 的每一列元素加起来得到 N 个数，把它们放在对角线上（其他地方都是零），组成一个 N\times N 的矩阵，记为 D 。并令 L = D-W 。  
求出 L 的前 k 个特征值（在本文中，除非特殊说明，否则“前 k 个”指按照特征值的大小从小到大的顺序）\{\lambda\}_{i=1}^k 以及对应的特征向量 \{\mathbf{v}\}_{i=1}^k 。  
把这 k 个特征（列）向量排列在一起组成一个 N\times k 的矩阵，将其中每一行看作 k 维空间中的一个向量，并使用 K-means 算法进行聚类。聚类的结果中每一行所属的类别就是原来 Graph 中的节点亦即最初的 N 个数据点分别所属的类别。

#### Hierarchical clustering
层次聚类，主要有两种，一种是bottom-up,另外一种是top-down；前面一种比较常见，把所有的点各成一类，然后通过平均距离或者最小距离，最大距离计算所有类之间的距离，把距离最小的两个类合并成一个类，重复过程，直到满足预设的簇数目。后者是比较启发式的算法，把所有点当做一类，在把这个大类拆分成两个小类是一个np问题，通常是把高于平均相似度的物品踢出去，独立成一个类。

#### DBSCAN
基于密度的聚类算法，首先把邻域内包含指定阈值样本的点抽出来，然后随机选一个，把这个点所有**样本可达**的点都视为一类，从数据集中剔除，然后重复上述过程。

#### Brich
一种类似与B- 树的层次聚类方法，适合大的数据集，只扫描了一次所有数据，每个节点存储了属于这个点的统计量(包括叶子节点数目，节点线性和，平方和)，因此比较节省内存，算法表述见下图，B- 树每次可以自行调整树的结构，保证平衡问题，详细参考[这里](http://blog.csdn.net/qll125596718/article/details/6895291)。
![brich](http://7xpv97.com1.z0.glb.clouddn.com/6ec6145a66e3293d5dc1781d96dfe694.png)
>Birch or MiniBatchKMeans?  
Birch does not scale very well to high dimensional data. As a rule of thumb if n_features is greater than twenty, it is generally better to use MiniBatchKMeans.  
If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.  

#### clustering evaluation
有很多判断聚类效果的指标，可以分为两种，一种是需要外部ground truth的，在实际操作中相对困难，具体有比如Rand Index， Adjusted Rand index(RI的正则版，区间是[-1,+1],+1表示完全相同)；卡方检验；互信息([0,1]，同样也有正则化版本，1表示最好)；homogeneity & completeness & V-measure（最容易计算的指标，但是没有正则化，参考随机划分的最差值）；另外一种是内部指标Silhouette Coefficientc，用来度量类的距离和大小之间的关系，通常而言，一个比较好点的聚类效果是类距离远，类较小，通过定义不同的距离函数还有多种的变化。
参考[这里](http://i11www.iti.uni-karlsruhe.de/extra/publications/ww-cco-06.pdf)

### Biclustring
中文名字叫双向聚类，和普通的聚类不同，普通的聚类只对一个维度做聚类，如果数据是一个二维矩阵的话，普通聚类可以在完成聚类后缩减聚类的某些列或者某些行；**双向聚类的话可以同时缩减某些列和某些行**，每个由这些列和行所组成的子矩阵成为子簇。和普通聚类不一样的是，可以定义子簇的含义，然后有不同的算法。sklearn给出的两种聚类都是基于谱聚类做的，目前也仅仅只有jaccard进行evaluation

### Decomposing signals in components

#### PCA
PCA是常用的降维方法，中文名字是主成分分析。本质是寻找一个超平面(直线的高维推广)，使得样本点到这个超平面的距离足够近，同时在这个超平面上的投影足够远。以下是算法描述：1.对所有样本进行中心化（正则化），2.计算协方差矩阵，3.对协方差矩阵进行特征值分解，4.取最大的k个特征值对应的特征向量作为投影矩阵输出。  
乍一看来，好像和LDA的思路有点像，但是区别还是明显的。LDA是分类算法，有监督；PCA是无监督的；LDA每次只能降一个维度，PCA可以降好多维，LDA的降维response to class label, 由于PCA是无监督的，并没有class label;由于特征值分解需要把整个矩阵装到内存里面，对大数据不友好，所以有对应的增量版本Incremental PCA；还有加速求解特征值的RandomizedPCA(内核是RandomizedSVD)，**特征值分解是奇异值分解的子集，参看wiki**，还有针对非线性空间的KernelPCA；还有加上L1正则化让数据更加稀疏的sparsePCA方法

#### LSA
也叫TruncatedSVD，Latent Semantic Analysis,对**原矩阵**而不是协方差矩阵进行**奇异值分解**，取k个最大的奇异值，重新生成矩阵，只保留了重要的维度信息。但是耗时也缺乏好的解释。

### 稀疏表示与字典学习
有些线性模型例如linear svm 需要数据是稀疏的来让问题变得线性可分；bow自带稀疏属性，而别的数据没有这种属性，只能通过字典学习DictionaryLearning来改善。这样一来除了稀疏向量以外还有字典要学习，可以加L1正则化然后交替下降(Coordinate Descent)。有梯度下降的问题就可以进行batch梯度下降进行改善。

#### Factor analysis
![factor analysis](http://7xpv97.com1.z0.glb.clouddn.com/85d88578b56af2c10c35cb9d298bc8cd.png)
见图所示。还有ICA就是非高斯prior的CA版本

#### Non-negative matrix factorization (NMF or NNMF)
字典学习也可以理解成一个矩阵分解的问题，这里加了一个限制，要求分解出来的两个矩阵都是非负的。sklearn里面目标函数是SE+L1+L2混合正则化， Coordinate Descent求解

#### LDA
LDA主题模型，问题表述相当类似，也是分解成两个矩阵，只是求解思路不一样

### 协方差矩阵估计
许多时候需要用到协方差矩阵，通过样本的协方差矩阵来预估整个变量空间的协方差矩阵，sklearn内置好多求协方差的方法，Empirical covariance是最简单的一种，通过MLE求解；如果样本很少，需要预估的变量很多，为了让矩阵可逆，可以在MLE上面加L2正则化和其余的一些近似算法；还有可能穾稀疏版本的协方差Sparse inverse covariance；还有为了抵抗数据扰动的Robust Covariance Estimation，主要是用来做离群点检测，下面会讲到这个问题。

### 离群点和异常点检测
区分离群点和异常点，离群点就是离原来的样本空间比较远，使用的方法有svm.OneClassSVM（只有一个类标号），异常点就是在一群点之中有少数类标号不同的点，异常点的检测主要是通过Least Median of Squares Regression([LSE](http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/LeastMedianOfSquares.pdf)),最小化误差平方的中位数，可以有效抵抗离群点
![LSE](http://7xpv97.com1.z0.glb.clouddn.com/3b933c7de5beb62f31f1c94e82895896.png)，通过LSE计算数据的分布，然后计算每个数据点跟分布的距离Mahalanobis distances，

### Density Estimation
用来估算密度，一种简单的技术就是直方图，切分成桶装然后绘成直方图，也可以用kernel方法绘制一个相对平滑的直方图，例如高斯核之类的。

### Neural Network
无监督神经网络模型

#### Restricted Boltzmann machines
Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. RBM是一种拓扑结构的称谓，其中包括使用能量函数。如图所示。
![RBM](http://7xpv97.com1.z0.glb.clouddn.com/49a709d74e121f7b106c8ee384ce3e57.png)
sklearn中只有BernoulliRBM，即所有的unit都是binary的。使用SML和PCD求解

### Cross Validation
备忘一下各种交叉验证形式

#### hold-out
从数据里选一部分作为训练集，其余作为测试集，计算模型的泛化能力

#### K-fold
把数据集分成k个部分(并不是放回采样)，每次用k-1个部分做训练集，对1个部分做测试。这样一来，**每个样本都会被预测一次**。

#### StratifiedKFold
是一种K-fold的变种，普通的K-fold不能保证训练集是否平衡，训练集里各个类标号的样本比例不平衡，StratifiedKFold就是为了解决这个问题。

#### Label k-fold
k-fold的变形，根据label进行交叉验证，测试集中不会出现训练集里没有出现过的label

#### Leave-One-Out - LOO
每次只测试一个样本，相当于有n个元素的数据集做n折交叉验证，是个相当耗时的计算方法；这种方法有很多变形，每次测试p个样本，称之为Leave-p-Out;还有根据label触发的Leave-One-Label-Out and Leave-P-Label-Out;

### GridSearch
sklearn带了一种tunning方法，需要调试的参数封装成一个list（里面是一个dict）,用GridSearch把需要tunning的Classifier一并传进来即可。还有参数不是枚举而是从分布里面获取的 RandomizedSearchCV。许多模型有内置的交叉验证方法来选择参数（感兴趣可以去看看代码），此外基于bagging的方法在生成训练集的时候自带Leave-p-Out的属性，可以不需要交叉验证。

### Metrics Score
#### classification
accuracy 预测值和准确值相同的个数，两个集合的交集的个数；Cohen’s kappa statistic；precision; recall; f1-score; hinge loss; Log loss; Matthews correlation coefficient; ROC; zero-one-loss;

#### mulit-label Score
多标签的classification给出概率形式的label判定，包括Coverage_error; Label ranking average precision; Rank Loss

#### Regression
预测值残差/真实值 Explained variance score； Mean absolute error(1-范式)；Mean squared error(2-范式); Median absolute error; R² score;

#### clustering
前面有提到的rank index； Adjusted ranked index; 还有内部距离等等；还有作为baseline的dummy classification

### Feature Extraction
简单备忘各种特征提取方法

#### categorical
相当于多标签，假设一共有三个标签可以表示成[1,0,0],[0,1,0],[0,0,1]

#### Feature hashing
一种降维的方法，设置一个定长的维度，将特征值的哈希值累加在这个定长的向量上面。可能可以用在文本降维上面。
举例如图所示，[参考](https://www.quora.com/Can-you-explain-feature-hashing-in-an-easily-understandable-way)
![feature hashing](http://7xpv97.com1.z0.glb.clouddn.com/8a73e1ddfdca47b51eaf6e83a01a55b6.png)

#### text feature
这个是相对熟悉的领域，首先是bag of word 模型(无视上下文顺序)，可以生成0/1 vector，0/count vector, 0/tf-idf vector；这个向量化的过程会受字典内存，并发困难等等限制，可以用上文提到的feature hashing,但是需要自己控制维度，无法逆转，无法用上idf信息；

#### image feature
image的区域region称之为patch，最简单的是patch Extraction,和文本的n-gram类似，提取出若干个相邻的矩阵。还有就是连通图矩阵，首先对矩阵进行聚类，然后属于一类的判定为联通，然后可以给出一个连通矩阵

### 数据预处理
主要是两种，一种是缩放，一种是平移，还有一种是正则化（使1-norm = 1 or 2-norm = 1）。RBF kernel的SVM或者L1，L2正则化的模型都需要数据在0周围分布，需要事先预处理**平移和缩放**到zero mean(**而不是聚集在[0,1]**)& one variance，sklearn还支持正则化到另外的区间[min,max], 添加缺失值的方法：均值，中位数，众数，或者SVD，clustering一下；sklearn 提供了多项式组合特征和自定义函数  
常见的数据预处理步骤是: 1. 清洗异常样本，离群点和异常样本；2.注意样本是否平衡；3.对单个特征可以归一化，离散化，编码，缺失值处理；4.对多个特征可以进行降维，包括Filter（相关系数，卡方检验，互信息），Wrapper（启发式搜索，GA，SA），Embedding(正则化，决策树，DL)；5.衍生变量


### 大规模语料学习
样本可以流式进入；提取特征不需要全局的状态，文本的bow特征可以用feature hashing 解决；模型支持增量学习,和部分特征学习，如图所示。
![out of core](http://7xpv97.com1.z0.glb.clouddn.com/af06917cfe6e21b6e5970cac0e247244.png)
