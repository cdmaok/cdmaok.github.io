---
layout: post
title: Js scrapy
category: Python
---

现在做爬虫的时候，会发现好多要爬取的都不是静态网页了，很多都是和js一起的，js代码进行异步读取操作。所以直接request.get("http://xxxx")这样的方法根本拿不到数据，一种方法是debug一下交互的过程，看看要的信息从那个js获取的，直接模拟对应的网络请求。  

本文的目的是介绍另外的一种方法，准确的说是一个库(python-qt4)，有了这个库，爬虫程序可以“假装”自己是个浏览器，知道自己要set-cookie之类的操作，要调用哪些js代码，完成完整的一次页面请求。  

首先安装库  

```
sudo apt-get install python-qt4
```
然后是实例代码  

```Python
import sys  
from PyQt4.QtGui import *  
from PyQt4.QtCore import *  
from PyQt4.QtWebKit import *  
from lxml import html

#Take this class for granted.Just use result of rendering.
class Render(QWebPage):  
  def __init__(self, url):  
    self.app = QApplication(sys.argv)  
    QWebPage.__init__(self)  
    self.loadFinished.connect(self._loadFinished)  
    self.mainFrame().load(QUrl(url))  
    self.app.exec_()  

  def _loadFinished(self, result):  
    self.frame = self.mainFrame()  
    self.app.quit()  

url = 'http://pycoders.com/archive/'  
r = Render(url)  
result = r.frame.toHtml()
#This step is important.Converting QString to Ascii for lxml to process
archive_links = html.fromstring(str(result.toAscii()))
print archive_links
```
这样完成一次交互之后，实际上执行了所有的js请求，再慢慢从页面里去拿数据即可。

Note: From [website](https://impythonist.wordpress.com/2015/01/06/ultimate-guide-for-scraping-javascript-rendered-web-pages/)
